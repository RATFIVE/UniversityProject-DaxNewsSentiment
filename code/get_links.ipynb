{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination about this Notebook\n",
    "In this notebook I gathered all the news links related to the DAX40 from the finanzen.net website.\n",
    "\n",
    "In the notebook `get_news.ipynb` I gathered the news from those links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium as se\n",
    "from time import sleep\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "# Driver for Firefox, Chrome, Edge, etc.\n",
    "from selenium import webdriver\n",
    "\n",
    "# Mode of locating html elements: ID, CSS_SELECTOR, XPATH, ...\n",
    "from selenium.webdriver.common.by import By               \n",
    "\n",
    "# Using specific keyboard keys like ENTER, ESCAPE, ...\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Methods for dropdown\n",
    "from selenium.webdriver.support.select import Select\n",
    "\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.finanzen.net/index/dax/marktberichte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Firefox browser to open the URL\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(url)\n",
    "\n",
    "# Set the time period for the driver to wait for the element to appear\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "def pop_up_window(driver, css_button, css_window):\n",
    "    try:\n",
    "        # Attempt to find and click the element\n",
    "        element = driver.find_element(By.CSS_SELECTOR, css_button)\n",
    "        element.click()\n",
    "    except:\n",
    "        #print(\"Element not found - Try to search in iframes\")\n",
    "\n",
    "        try:\n",
    "            # Accept the Datenschutz\n",
    "            iframes = driver.find_elements(By.CSS_SELECTOR, css_window)\n",
    "            #print(f\"Number of iframes: {len(iframes)}\")\n",
    "\n",
    "            for i in range(len(iframes)):\n",
    "                try:\n",
    "                    driver.switch_to.frame(iframes[i])\n",
    "                    #print(f\"Switched to Pop-Up {i}\")\n",
    "                    \n",
    "                    # Attempt to find and click the element\n",
    "                    element = driver.find_element(By.CSS_SELECTOR, css_button)\n",
    "                    element.click()\n",
    "                    \n",
    "                    #print(\"Pop-Up found and clicked\")\n",
    "                    break  # Exit the loop once the element is found and clicked\n",
    "                except NoSuchElementException:\n",
    "                    #print(f\"Element not found in Pop-Up {i}\")\n",
    "                    pass\n",
    "                except ElementNotInteractableException:\n",
    "                    # print(f\"Element not interactable in Pop-Up {i}\")\n",
    "                    pass\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred in Pop-Up {i}: {str(e)}\")\n",
    "                finally:\n",
    "                    # Switch back to the main content before checking the next iframe\n",
    "                    driver.switch_to.default_content()\n",
    "        except Exception as e:\n",
    "            print(f'Could not find the Pop-Up button: {str(e)}')\n",
    "\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = pop_up_window(driver, css_button=\".button-responsive-primary\", css_window=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n"
     ]
    }
   ],
   "source": [
    "# look for the highest page in the pagination\n",
    "css_selector = 'li.pagination__item:nth-child(11) > a:nth-child(1)'\n",
    "max_pages = driver.find_element(By.CSS_SELECTOR, css_selector)\n",
    "print(max_pages.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(driver, url):\n",
    "    all_links = pd.DataFrame(columns=['links'])\n",
    "    for page in tq(range(1, int(max_pages.text)), desc='Pages'):\n",
    "\n",
    "        driver = pop_up_window(driver, css_button=\".finWebpushCloseButton\", css_window=\"#finWebpushNotificationModal\")\n",
    "\n",
    "        article_list = []\n",
    "        url_page =url + f'?p={page}'\n",
    "        driver.get(url_page)\n",
    "        #driver.implicitly_wait(10)\n",
    "        source = driver.page_source\n",
    "        soup = BeautifulSoup(source, 'html.parser') \n",
    "        # find article\n",
    "        \n",
    "        articles = soup.find_all('article')\n",
    "        for article in articles:\n",
    "            article_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            links = article_soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                # print(link)\n",
    "                # print(link['href'])\n",
    "                article_list.append(link['href'])\n",
    "                \n",
    "        # print(f'Page {page} - {len(article_list)} articles found')\n",
    "        # drop hrefs wich start with https://\n",
    "        article_list = [x for x in article_list if not x.startswith('https://')]\n",
    "        article_list = pd.DataFrame(article_list, columns=['links'])\n",
    "        all_links = pd.concat([all_links, article_list], ignore_index=True)\n",
    "\n",
    "        # safty first\n",
    "        if page % 10 == 0: # save every 10 pages\n",
    "            dir_name = 'links'\n",
    "            all_links.to_csv(f'..\\\\data\\\\links\\\\links-{page}.csv', index=False)\n",
    "            pass\n",
    "\n",
    "        # if page == 3:\n",
    "        #     break\n",
    "            \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pages: 100%|██████████| 424/424 [2:35:57<00:00, 22.07s/it]  \n"
     ]
    }
   ],
   "source": [
    "article_list = get_news_links(driver=driver, url=url)\n",
    "# This code was running for 2:30 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40290"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'drop_duplicates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# check for duplicates in article_list\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_temp \u001b[38;5;241m=\u001b[39m article_list\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdf_temp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# drop link if it contains '?p='\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_temp \u001b[38;5;241m=\u001b[39m df_temp[\u001b[38;5;241m~\u001b[39mdf_temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m?p=\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'drop_duplicates'"
     ]
    }
   ],
   "source": [
    "# check for duplicates in article_list\n",
    "df_temp = article_list.copy()\n",
    "df_temp.drop_duplicates(inplace=True)\n",
    "# drop link if it contains '?p='\n",
    "df_temp = df_temp[~df_temp['links'].str.contains('\\?p=')]\n",
    "df_temp.reset_index(drop=True, inplace=True)\n",
    "df_temp.to_csv(r'..\\data\\links\\article_list.tsv', index=False, sep='\\t')\n",
    "article_list = df_temp['links'].tolist()\n",
    "print(f'len(article_list): {len(article_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue reading the code please look at `get_news.iypnb`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SMA",
   "language": "python",
   "name": ".sma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
