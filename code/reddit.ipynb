{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import pandas as pd\n",
    "import redditwarp.SYNC\n",
    "from praw.models import MoreComments\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv('reddit.env')\n",
    "TOKEN = os.getenv('TOKEN')\n",
    "SECRET = os.getenv('SECRET')\n",
    "USER_AGEND = os.getenv('USER_AGEND')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client instance\n",
    "client = redditwarp.SYNC.Client()\n",
    "\n",
    "# Create a reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=TOKEN, \n",
    "    client_secret=SECRET, \n",
    "    user_agent=USER_AGEND\n",
    ")\n",
    "\n",
    "# Set read only\n",
    "reddit.read_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utc_timestamp(date_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Converts a date string to a UTC Unix timestamp (seconds since the epoch).\n",
    "\n",
    "    Args:\n",
    "        date_str (str): A string representing the date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        int: The corresponding Unix timestamp (in seconds) for the input date, assuming midnight UTC.\n",
    "\n",
    "    The function parses the provided date string, assumes midnight for the time, sets the timezone to UTC, \n",
    "    and converts it to a Unix timestamp, which represents the number of seconds since January 1, 1970.\n",
    "    \"\"\"\n",
    "\n",
    "    local_time = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "    # Set the timezone to UTC\n",
    "    utc_time = local_time.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # Convert to a Unix timestamp\n",
    "    timestamp = int(utc_time.timestamp())\n",
    "\n",
    "    return timestamp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submissions(subreddits:list, search_words:list, start_date:int, end_date:int)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches submissions from specified subreddits that contain specific search words and fall within a given date range.\n",
    "\n",
    "    Args:\n",
    "        subreddits (list): A list of subreddit names (strings) to search in.\n",
    "        search_word (list): A list of search words (strings) to filter the submissions.\n",
    "        start_date (int): The start date as a Unix timestamp (seconds since the epoch).\n",
    "        end_date (int): The end date as a Unix timestamp (seconds since the epoch).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing submission data filtered by subreddit, search word, and date range. \n",
    "                      The DataFrame has the following columns:\n",
    "                      - 'id': The submission ID.\n",
    "                      - 'date': The submission's creation date as a string in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "                      - 'title': The submission's title.\n",
    "                      - 'score': The submission's score.\n",
    "                      - 'subreddit': The name of the subreddit.\n",
    "                      - 'topic': The search word/topic used to find the submission.\n",
    "\n",
    "    This function searches through each subreddit for posts matching the provided search words. It retrieves up to 25\n",
    "    submissions per search word, filtering by posts that were created within the specified date range. The results are\n",
    "    returned as a DataFrame, with each submission's key information extracted and stored. The function pauses for 1 second\n",
    "    between each post retrieval to avoid hitting rate limits.\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    for subreddit in tqdm(subreddits, desc='Get Subreddits:'):\n",
    "        for word in search_words: \n",
    "            i = 0\n",
    "            for submission in reddit.subreddit(subreddit).search(word):\n",
    "                date = submission.created_utc\n",
    "                if (date >= start_date) and (date <= end_date):\n",
    "\n",
    "                    data = {}\n",
    "                    timestamp = datetime.fromtimestamp(date)\n",
    "\n",
    "                    # Get the submission data\n",
    "                    data[\"id\"] = submission.id\n",
    "                    data['date'] = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    data[\"title\"] = submission.title\n",
    "                    data['score'] = submission.score\n",
    "                    data['subreddit'] = subreddit\n",
    "                    data['topic'] = word\n",
    "\n",
    "                    result.append(data)\n",
    "                    i += 1\n",
    "\n",
    "                    # Just get 25 submissions for each search word in each subreddit\n",
    "                    if i >= 25:\n",
    "                        break\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "    df_result = pd.DataFrame(result).sort_values(by='score', ascending=False).drop_duplicates(subset='id', keep='first')\n",
    "    # keep ontly submissions with score > 10\n",
    "    df_result = df_result[df_result['score'] > 10]\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches the top 25 comments for each post in the provided DataFrame based on the post IDs and \n",
    "    returns a DataFrame with the post IDs and their corresponding comments.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame that contains at least an 'id' column with the post IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with two columns:\n",
    "                    - 'id': The post IDs.\n",
    "                    - 'comments': A list of up to 25 top comments for each post. If a comment is \n",
    "                        deleted or removed, it is replaced with an empty string.\n",
    "    \n",
    "    The function iterates through the post IDs in the input DataFrame, fetches the top 25 comments \n",
    "    for each post, and handles cases where comments are deleted, removed, or if the request times out.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for x, id in tqdm(enumerate(df['id']), desc='Get comments', total=len(df['id'])):\n",
    "        try:\n",
    "            comments_data = []\n",
    "            tree_node = client.p.comment_tree.fetch(id, sort='top', limit=100)\n",
    "\n",
    "            # Get the top 25 comments\n",
    "            for i in range(50):\n",
    "                if i < len(tree_node.children):\n",
    "                    c = tree_node.children[i].value\n",
    "\n",
    "                    # Check if the comment is not deleted or removed\n",
    "                    if c.body not in ['[deleted]', '[removed]', 'Removed by Reddit']:\n",
    "                        comments_data.append(c.body)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            data = {'id': id, \n",
    "                    'comments': comments_data}\n",
    "            results.append(data)\n",
    "            \n",
    "        except TimeoutError:\n",
    "            print(\"TimeoutError\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    df_comments = pd.DataFrame(results)\n",
    "\n",
    "    return df_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(comments: list) -> list:\n",
    "    \"\"\"\n",
    "    Cleans the comments by removing new line characters, extra spaces, and converting the text to lowercase.\n",
    "\n",
    "    Args:\n",
    "        comments (list): A list of comments (strings) to clean.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned comments with new line characters removed, extra spaces removed, and text converted to lowercase.\n",
    "    \"\"\"\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        if '**User Report**' in comment:\n",
    "            continue\n",
    "        cleaned_comment = comment.replace('\\n', ' ').strip()\n",
    "       \n",
    "        cleaned_comments.append(cleaned_comment)\n",
    "    return cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Subreddits:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [18:44<00:00, 1124.75s/it]\n",
      "Get comments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 649/649 [09:19<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>topic</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1g3bvom</td>\n",
       "      <td>2024-10-14 11:33:27</td>\n",
       "      <td>Salarii Continental internship</td>\n",
       "      <td>32</td>\n",
       "      <td>All</td>\n",
       "      <td>Continental</td>\n",
       "      <td>[Wow ai expus o situatie ca in orice corporati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1g3903i</td>\n",
       "      <td>2024-10-14 07:33:26</td>\n",
       "      <td>Income-Allianz deal off; government assesses i...</td>\n",
       "      <td>1321</td>\n",
       "      <td>All</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>[These are the people who cared enough to voic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1g2iikt</td>\n",
       "      <td>2024-10-13 07:15:12</td>\n",
       "      <td>I walked from Mexico to Canada on the Continen...</td>\n",
       "      <td>14769</td>\n",
       "      <td>All</td>\n",
       "      <td>Continental</td>\n",
       "      <td>[Would love to know a more detailed story, foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1g26mn5</td>\n",
       "      <td>2024-10-12 20:33:48</td>\n",
       "      <td>Nature‚Äôs Harvest: Capturing the Flow of Rubber...</td>\n",
       "      <td>15271</td>\n",
       "      <td>All</td>\n",
       "      <td>SAP</td>\n",
       "      <td>[What's the purpose of the first vertical cut?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1g1gs54</td>\n",
       "      <td>2024-10-11 20:35:51</td>\n",
       "      <td>First Offer! 2025 Grad at Deutsche Bank</td>\n",
       "      <td>107</td>\n",
       "      <td>All</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>[I keep trying to spread the word about bank d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1g1ekwq</td>\n",
       "      <td>2024-10-11 18:59:25</td>\n",
       "      <td>Rheinmetall CEO Says Arms Boom Is the Biggest ...</td>\n",
       "      <td>1686</td>\n",
       "      <td>All</td>\n",
       "      <td>Rheinmetall</td>\n",
       "      <td>[The biggest he's ever seen, so far, every non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1g19p5w</td>\n",
       "      <td>2024-10-11 15:23:26</td>\n",
       "      <td>Sartorius layoffs</td>\n",
       "      <td>79</td>\n",
       "      <td>All</td>\n",
       "      <td>Sartorius</td>\n",
       "      <td>[their instruments break all the damn time, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1g0y5th</td>\n",
       "      <td>2024-10-11 03:07:03</td>\n",
       "      <td>Thrifted Adidas and Nike Hoodies I combine</td>\n",
       "      <td>6271</td>\n",
       "      <td>All</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>[Join our Discord here: https://discord.gg/6aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1g0mhd0</td>\n",
       "      <td>2024-10-10 18:12:40</td>\n",
       "      <td>Deutsche Post erh√§lt Auszeichnung als beste Po...</td>\n",
       "      <td>1975</td>\n",
       "      <td>All</td>\n",
       "      <td>Deutsche Post</td>\n",
       "      <td>[EIne Preisverleihungsgala des Weltpostvereins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1g0ew6u</td>\n",
       "      <td>2024-10-10 11:31:06</td>\n",
       "      <td>Rheiner im Sale üî•</td>\n",
       "      <td>14</td>\n",
       "      <td>All</td>\n",
       "      <td>Rheinmetall</td>\n",
       "      <td>[Rheiner macht halt Rheiner SachenüòÅ , ist v√∂ll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                 date  \\\n",
       "576  1g3bvom  2024-10-14 11:33:27   \n",
       "337  1g3903i  2024-10-14 07:33:26   \n",
       "89   1g2iikt  2024-10-13 07:15:12   \n",
       "88   1g26mn5  2024-10-12 20:33:48   \n",
       "509  1g1gs54  2024-10-11 20:35:51   \n",
       "314  1g1ekwq  2024-10-11 18:59:25   \n",
       "534  1g19p5w  2024-10-11 15:23:26   \n",
       "174  1g0y5th  2024-10-11 03:07:03   \n",
       "296  1g0mhd0  2024-10-10 18:12:40   \n",
       "632  1g0ew6u  2024-10-10 11:31:06   \n",
       "\n",
       "                                                 title  score subreddit  \\\n",
       "576                     Salarii Continental internship     32       All   \n",
       "337  Income-Allianz deal off; government assesses i...   1321       All   \n",
       "89   I walked from Mexico to Canada on the Continen...  14769       All   \n",
       "88   Nature‚Äôs Harvest: Capturing the Flow of Rubber...  15271       All   \n",
       "509            First Offer! 2025 Grad at Deutsche Bank    107       All   \n",
       "314  Rheinmetall CEO Says Arms Boom Is the Biggest ...   1686       All   \n",
       "534                                  Sartorius layoffs     79       All   \n",
       "174        Thrifted Adidas and Nike Hoodies I combine    6271       All   \n",
       "296  Deutsche Post erh√§lt Auszeichnung als beste Po...   1975       All   \n",
       "632                                  Rheiner im Sale üî•     14       All   \n",
       "\n",
       "             topic                                           comments  \n",
       "576    Continental  [Wow ai expus o situatie ca in orice corporati...  \n",
       "337        Allianz  [These are the people who cared enough to voic...  \n",
       "89     Continental  [Would love to know a more detailed story, foo...  \n",
       "88             SAP  [What's the purpose of the first vertical cut?...  \n",
       "509  Deutsche Bank  [I keep trying to spread the word about bank d...  \n",
       "314    Rheinmetall  [The biggest he's ever seen, so far, every non...  \n",
       "534      Sartorius  [their instruments break all the damn time, I ...  \n",
       "174         Adidas  [Join our Discord here: https://discord.gg/6aa...  \n",
       "296  Deutsche Post  [EIne Preisverleihungsgala des Weltpostvereins...  \n",
       "632    Rheinmetall  [Rheiner macht halt Rheiner SachenüòÅ , ist v√∂ll...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649, 7)\n"
     ]
    }
   ],
   "source": [
    "start_date = get_utc_timestamp('2017-01-01')\n",
    "end_date = get_utc_timestamp('2024-10-15')\n",
    "\n",
    "dax40_companies = [\n",
    "    \"Adidas\", \"Airbus\", \"Allianz\", \"BASF\", \"Bayer\", \"Beiersdorf\", \n",
    "    \"BMW\", \"Brenntag\", \"Commerzbank\", \"Continental\", \"Covestro\", \n",
    "    \"Daimler Truck\", \"Deutsche Bank\", \"Deutsche B√∂rse\", \"Deutsche Post\", \n",
    "    \"Deutsche Telekom\", \"E.ON\", \"Fresenius\", \"Fresenius Medical Care\", \n",
    "    \"Hannover R√ºck\", \"HeidelbergCement\", \"Henkel\", \"Infineon\", \"Linde\", \n",
    "    \"Mercedes-Benz Group\", \"Merck\", \"MTU Aero Engines\", \"M√ºnchener R√ºck\", \n",
    "    \"Porsche AG\", \"Puma\", \"Qiagen\", \"Rheinmetall\", \"RWE\", \"SAP\", \"Sartorius\", \n",
    "    \"Siemens\", \"Siemens Healthineers\", \"Symrise\", \"Volkswagen\", \"Zalando\", 'Muenchener Rueck', 'Deutsche Boerse', 'Hannover Rueck'\n",
    "]\n",
    "\n",
    "#subreddits = ['Aktien', 'news', 'worldnews', 'stocks', 'wallstreetbets', 'finance', 'germany', 'market', 'stockmarket', 'investing', 'europe', 'economy', 'business', 'trading', 'phinvest']\n",
    "search_word = ['DAX', 'DAX40', 'DAX30'] + dax40_companies\n",
    "subreddits = ['All']\n",
    "search_word = ['DAX40', 'DAX30'] + dax40_companies\n",
    "\n",
    "\n",
    "df_submissions = get_submissions(subreddits, search_word, start_date, end_date)\n",
    "df_comments = get_comments(df_submissions)\n",
    "\n",
    "df_merged = pd.merge(df_submissions, df_comments, on='id').sort_values(by='date', ascending=False)\n",
    "df_merged['comments'] = df_merged['comments'].apply(clean_comments)\n",
    "display(df_merged.head(10))\n",
    "print(df_merged.shape)\n",
    "# This code was rinning for 2 hurs and 30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('reddit_all_data.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main",
   "language": "python",
   "name": ".main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
